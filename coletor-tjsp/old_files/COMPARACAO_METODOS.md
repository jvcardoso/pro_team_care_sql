# üìä Compara√ß√£o: Web Scraping vs API DataJud

## üéØ Qual M√©todo Usar?

Baseado no seu projeto atual e nas suas necessidades, aqui est√° uma an√°lise completa.

---

## üìã Tabela Comparativa Detalhada

| Aspecto | Web Scraping (Atual) | API DataJud (CNJ) |
|---------|---------------------|-------------------|
| **Legalidade** | ‚úÖ Legal (dados p√∫blicos) | ‚úÖ 100% Legal (API oficial) |
| **Custo** | Gratuito | Gratuito |
| **Cadastro** | N√£o requer | N√£o requer |
| **Chave de Acesso** | N√£o precisa | P√∫blica (j√° dispon√≠vel) |
| **Complexidade** | M√©dia-Alta | Baixa |
| **Manuten√ß√£o** | Alta (site muda) | Baixa (API est√°vel) |
| **Velocidade** | Lenta (10-15s/processo) | R√°pida (1-2s/processo) |
| **CAPTCHA** | ‚ö†Ô∏è Pode bloquear | ‚úÖ Sem CAPTCHA |
| **Rate Limiting** | Sim (6 req/min) | N√£o documentado |
| **Atualiza√ß√£o** | Tempo real | Delay de horas/dias |
| **Cobertura** | Apenas TJSP | Todos os tribunais |
| **Dados Dispon√≠veis** | Completos | Metadados |
| **Documentos (PDFs)** | ‚úÖ Poss√≠vel | ‚ùå N√£o dispon√≠vel |
| **Confiabilidade** | 70-80% | 95-99% |

---

## üîç An√°lise Detalhada

### 1. **Dados Dispon√≠veis**

#### Web Scraping (TJSP)
```
‚úÖ Metadados do processo
‚úÖ Partes e advogados
‚úÖ Movimenta√ß√µes completas
‚úÖ Decis√µes e despachos
‚úÖ Valores atualizados
‚úÖ Links para documentos
‚úÖ Informa√ß√µes espec√≠ficas do TJSP
```

#### API DataJud
```
‚úÖ Metadados do processo
‚úÖ Partes e advogados
‚úÖ Movimenta√ß√µes (resumidas)
‚úÖ Classe e assunto
‚úÖ √ìrg√£o julgador
‚ùå Documentos (PDFs)
‚ùå Detalhes espec√≠ficos do tribunal
```

### 2. **Performance**

#### Web Scraping
- **Tempo por processo:** 10-15 segundos
- **Processos/hora:** ~200-300
- **Gargalo:** Navega√ß√£o do browser, CAPTCHA
- **Consumo de recursos:** Alto (browser completo)

#### API DataJud
- **Tempo por processo:** 1-2 segundos
- **Processos/hora:** ~1.800-3.600
- **Gargalo:** Rate limiting (se houver)
- **Consumo de recursos:** Baixo (apenas HTTP)

### 3. **Confiabilidade**

#### Web Scraping
- ‚ö†Ô∏è Sens√≠vel a mudan√ßas no site
- ‚ö†Ô∏è CAPTCHA pode bloquear
- ‚ö†Ô∏è Requer manuten√ß√£o constante
- ‚úÖ Dados sempre atualizados

#### API DataJud
- ‚úÖ API est√°vel e documentada
- ‚úÖ Sem risco de bloqueio
- ‚úÖ Sem manuten√ß√£o de seletores
- ‚ö†Ô∏è Dados podem ter delay

---

## üéØ Recomenda√ß√µes por Caso de Uso

### Caso 1: Monitoramento de Processos Espec√≠ficos
**Recomenda√ß√£o:** Web Scraping (Atual)

**Motivo:**
- Precisa de dados em tempo real
- N√∫mero limitado de processos
- Necessita de detalhes completos

**Exemplo:**
```python
# Monitorar 50 processos espec√≠ficos do condom√≠nio
python run.py --input processos_condominio.csv --output output/
```

---

### Caso 2: Pesquisa e An√°lise em Massa
**Recomenda√ß√£o:** API DataJud

**Motivo:**
- Precisa consultar centenas/milhares de processos
- An√°lise estat√≠stica
- N√£o precisa de documentos

**Exemplo:**
```python
# Buscar todos os processos de um advogado
api = DataJudAPI('tjsp')
processos = api.consultar_por_parte('Adilson Lopes Teixeira', max_resultados=1000)
```

---

### Caso 3: Dashboard e Relat√≥rios
**Recomenda√ß√£o:** H√≠brida

**Motivo:**
- Use DataJud para listar processos
- Use Web Scraping para detalhes espec√≠ficos
- Melhor custo-benef√≠cio

**Exemplo:**
```python
# 1. Buscar processos na API DataJud (r√°pido)
processos = api.consultar_por_parte('Rio Nieva')

# 2. Para cada processo, buscar detalhes via scraping (quando necess√°rio)
for proc in processos_prioritarios:
    detalhes = scraper.search_by_process_number(proc['numero'])
```

---

### Caso 4: An√°lise Multi-Tribunal
**Recomenda√ß√£o:** API DataJud

**Motivo:**
- Acesso centralizado a todos os tribunais
- Dados padronizados
- Imposs√≠vel via web scraping (cada tribunal √© diferente)

**Exemplo:**
```python
# Buscar processos em m√∫ltiplos tribunais
tribunais = ['tjsp', 'tjrj', 'tjmg', 'trf3']
for tribunal in tribunais:
    api = DataJudAPI(tribunal)
    processos = api.consultar_por_parte('Empresa XYZ')
```

---

## üí° Estrat√©gia H√≠brida (Recomendada)

### Arquitetura Proposta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           1. Busca Inicial (DataJud)            ‚îÇ
‚îÇ  - Listar todos os processos                    ‚îÇ
‚îÇ  - Filtrar por crit√©rios                        ‚îÇ
‚îÇ  - Identificar processos de interesse           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      2. Cache e Verifica√ß√£o de Novidades        ‚îÇ
‚îÇ  - Comparar com processos j√° coletados          ‚îÇ
‚îÇ  - Identificar novos processos                  ‚îÇ
‚îÇ  - Identificar processos atualizados            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    3. Coleta Detalhada (Web Scraping TJSP)      ‚îÇ
‚îÇ  - Apenas para processos novos/atualizados      ‚îÇ
‚îÇ  - Coletar movimenta√ß√µes completas              ‚îÇ
‚îÇ  - Extrair documentos e anexos                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         4. Armazenamento e An√°lise              ‚îÇ
‚îÇ  - Consolidar dados de ambas as fontes          ‚îÇ
‚îÇ  - Gerar dashboard                              ‚îÇ
‚îÇ  - Criar relat√≥rios                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementa√ß√£o

```python
class ColetorHibrido:
    """Combina API DataJud com Web Scraping TJSP"""
    
    def __init__(self):
        self.api = DataJudAPI('tjsp')
        self.scraper = TJSPScraper()
        self.cache = ProcessoCache()
    
    async def coletar_processos(self, nome_parte: str):
        """Coleta h√≠brida: API + Scraping"""
        
        # 1. Buscar lista na API (r√°pido)
        print("1. Buscando processos na API DataJud...")
        processos_api = self.api.consultar_por_parte(nome_parte, max_resultados=100)
        print(f"   ‚úÖ {len(processos_api)} processos encontrados")
        
        # 2. Filtrar processos que precisam de atualiza√ß√£o
        print("\n2. Verificando cache...")
        processos_atualizar = []
        
        for proc in processos_api:
            if self.cache.precisa_atualizar(proc['numero']):
                processos_atualizar.append(proc['numero'])
        
        print(f"   ‚úÖ {len(processos_atualizar)} processos precisam de atualiza√ß√£o")
        
        # 3. Coletar detalhes via scraping (apenas necess√°rios)
        print("\n3. Coletando detalhes via web scraping...")
        
        async with self.scraper:
            for i, numero in enumerate(processos_atualizar, 1):
                print(f"   [{i}/{len(processos_atualizar)}] {numero}")
                
                detalhes = await self.scraper.search_by_process_number(numero)
                
                if detalhes:
                    # Mesclar dados da API com dados do scraping
                    processo_completo = self._mesclar_dados(
                        next(p for p in processos_api if p['numero'] == numero),
                        detalhes
                    )
                    
                    self.cache.salvar(processo_completo)
                
                await self.scraper.random_delay()
        
        print("\n‚úÖ Coleta h√≠brida conclu√≠da!")
        return self.cache.listar_todos()
    
    def _mesclar_dados(self, dados_api: dict, dados_scraping: dict) -> dict:
        """Combina dados de ambas as fontes"""
        return {
            **dados_api,  # Dados da API
            'movimentacoes_completas': dados_scraping.get('movimentacoes', []),
            'documentos': dados_scraping.get('documentos', []),
            'detalhes_tjsp': dados_scraping
        }
```

---

## üìà An√°lise de Custos

### Cen√°rio: 1.000 Processos

#### Apenas Web Scraping
- **Tempo:** ~4-5 horas
- **Risco de bloqueio:** Alto
- **Manuten√ß√£o:** Alta
- **Custo computacional:** Alto

#### Apenas API DataJud
- **Tempo:** ~30 minutos
- **Risco de bloqueio:** Zero
- **Manuten√ß√£o:** Baixa
- **Limita√ß√£o:** Dados incompletos

#### H√≠brido (Recomendado)
- **Tempo:** ~2 horas
- **Risco de bloqueio:** Baixo
- **Manuten√ß√£o:** M√©dia
- **Resultado:** Dados completos

---

## ‚úÖ Decis√£o Final

### Para Seu Projeto Espec√≠fico

Baseado no seu c√≥digo atual (`coletor-tjsp`) e nos dashboards gerados, recomendo:

#### **Fase 1: Manter Web Scraping (Curto Prazo)**
- ‚úÖ Seu c√≥digo j√° funciona
- ‚úÖ Gera dashboards completos
- ‚úÖ Dados espec√≠ficos do TJSP

**Melhorias:**
1. Implementar cache robusto
2. Melhorar detec√ß√£o de CAPTCHA
3. Adicionar retry autom√°tico

#### **Fase 2: Adicionar API DataJud (M√©dio Prazo)**
- ‚úÖ Usar para busca inicial de processos
- ‚úÖ Reduzir carga no site TJSP
- ‚úÖ Aumentar velocidade de coleta

**Implementa√ß√£o:**
1. Criar m√≥dulo `src/datajud_client.py`
2. Integrar com c√≥digo existente
3. Usar cache para evitar duplica√ß√£o

#### **Fase 3: Abordagem H√≠brida (Longo Prazo)**
- ‚úÖ Melhor dos dois mundos
- ‚úÖ M√°xima efici√™ncia
- ‚úÖ Dados completos

---

## üöÄ Pr√≥ximos Passos

### 1. Testar API DataJud
```bash
cd /home/juliano/Projetos/meu_projeto/coletor-tjsp
python exemplo_datajud.py
```

### 2. Comparar Resultados
- Execute coleta via web scraping
- Execute coleta via API DataJud
- Compare qualidade dos dados

### 3. Decidir Estrat√©gia
- Se API atende: Migre gradualmente
- Se precisa de mais: Implemente h√≠brido
- Se espec√≠fico TJSP: Otimize scraping atual

---

## üìö Recursos

### Documenta√ß√£o
- **API DataJud:** https://datajud-wiki.cnj.jus.br/api-publica/
- **Seu Projeto:** `ANALISE_VIABILIDADE_LEGAL.md`
- **Exemplo Pr√°tico:** `exemplo_datajud.py`

### Suporte
- **Issues GitHub:** Procure "datajud python"
- **Comunidade:** Stack Overflow tag `datajud`

---

**Conclus√£o:** A melhor estrat√©gia √© **h√≠brida**, usando API DataJud para busca inicial e Web Scraping para detalhes espec√≠ficos. Isso maximiza velocidade, confiabilidade e completude dos dados.

**√öltima atualiza√ß√£o:** 30/10/2025
